import techminer as tech

#
# Converts scopus.csv to techminer.csv
#
tech.import_scopus()


import techminer as tech

#
# Creates or modifies the keywords thesaurus.txt
#   Honors previous created file
#
tech.create_keywords_thesaurus()

get_ipython().getoutput("head -n +20 keywords_thesaurus.txt")


import techminer as tech

#
# Keywords cleaning
#
tech.apply_keywords_thesaurus()


import techminer as tech

#
# Creates a Institutions thesaurus
#
tech.create_institutions_thesaurus()

get_ipython().getoutput("head -n +10 institutions_thesaurus.txt")


import techminer as tech

#
# Institutions cleaning
#
tech.apply_institutions_thesaurus()


import techminer as tech

#
# Coverage
#
tech.coverage()


import techminer as tech

#
# Descriptive stats
#
tech.descriptive_stats()


import techminer as tech

#
# Column explorer
#
tech.column_explorer(top_n=350, only_abstract=True, clusters=None, cluster=4)


import techminer as tech

#
# Matrix explorer
#
tech.matrix_explorer(top_n=100, only_abstract=True, clusters=None, cluster=4)


import techminer as tech

#
# By year analysis
#
tech.by_year_analysis(clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Top documents report
#
tech.top_documents(top_n=10, clusters=CLUSTERS, cluster=1)


import techminer as tech

#
# By term analysis
#
tech.by_term_analysis(
    limit_to=None, exclude=None, years_range=None, clusters=CLUSTERS, cluster=0
)


import techminer as tech

#
# By-term-per-year
#
tech.by_term_per_year_analysis(tab=0, years_range=None, clusters=CLUSTERS, cluster=3)


import techminer as tech

#
# Growth indicators
#
tech.growth_indicators(exclude=None, years_range=None, clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Bigraph
#
tech.bigraph_analysis(clusters=CLUSTERS, cluster=1)


import techminer as tech

#
# Graph
#
tech.graph_analysis(exclude=None, clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Correlation
#
tech.correlation_analysis(clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Factor Analysis
#
tech.factor_analysis(clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Concept Mapping
#
tech.concept_mapping(clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Latent Semantic Analysis
#
tech.latent_semantic_analysis(clusters=CLUSTERS, cluster=0)


import techminer as tech

#
# Comparative Analysis
#
tech.comparative_analysis(clusters=CLUSTERS, cluster=0)


#
# Clusters
#

CLUSTERS = [
    "Author_Keywords_CL",
    {
        0: [
            "city logistic",
            "urban freight transport",
            "urban freight consolidation center",
            "case studies",
            "stakeholder",
            "game theory",
            "optimization",
            "multi-stakeholder",
        ],
        1: [
            "urban distribution center",
            "sustainability",
            "location selection",
            "distribution center",
            "city logistics centers",
            "multi criteria decision making",
            "location plan",
        ],
        2: [
            "urban consolidation center",
            "business model",
            "last mile delivery",
            "urban distribution",
            "urban freight",
            "freight consolidation",
        ],
        3: [
            "logistics",
            "city distribution center",
            "collaboration",
            "entropy",
            "facility location",
        ],
        4: [
            "urban logistic",
            "freight transport",
            "electric vehicle",
            "cities",
        ],
    },
]


import techminer as tech

#
# Thematic Analysis
#
tech.thematic_analysis(clusters=CLUSTERS, cluster=2)


import techminer as tech

#
# Document-term
#
tech.document_term_analysis()


##
## Limit to ... / Exclude ...
##
LIMIT_TO = {
    "Author_Keywords": text.find_string(
        patterns="^g",
        x=df.Author_Keywords,
        ignore_case=True,
        full_match=False,
        use_re=True,
    ),
    "Countries": ["United States"],
}

EXCLUDE = {
    "Author_Keywords": text.find_string(
        patterns="^g",
        x=df.Author_Keywords,
        ignore_case=True,
        full_match=False,
        use_re=True,
    ),
    "Countries": ["United States"],
    "Authors": ["Utzinger J", "Polling B"],
}


import pandas as pd

data = pd.read_csv("techminer.csv")
for s in data.Abstract_phrase_words:
    print(s)
    print()


import pandas as pd

data = pd.read_csv("techminer.csv")

for i in range(20):
    print(data.Title_words[i])
    print("")
    print("")


import nltk

nltk.pos_tag(["the", "findings", "are", "unique", "carriers"])


import nltk

nltk.pos_tag(["findings"])


import nltk

nltk.download("tagsets")
nltk.help.upenn_tagset()


import pandas as pd

data = pd.read_csv("techminer.csv")
for s in data.Abstract[0:10]:
    print(s.lower())
    print()

import pandas as pd

data = pd.read_csv("scopus.csv")
data = data[0:10]
data.to_csv("scopus_.csv", index=False)


import pandas as pd

data = pd.read_csv("techminer.csv")
for s in data.Abstract:
    print(s)
    print()


from nltk.stem import WordNetLemmatizer

wordnet_lemmatizer = WordNetLemmatizer()
wordnet_lemmatizer.lemmatize("activities", "v")


import nltk

nltk.__version__


from textblob import Word

Word("areas").singularize()


import pandas as pd

data = pd.read_csv("techminer.csv")
for s in data.Abstract_phrase_words:
    print(repr(s))
    print()


import pandas as pd

data = pd.read_csv("techminer.csv")
for s in data.Keywords_CL:
    print(repr(s))
    print()


from nltk.tokenize import word_tokenize

word_tokenize("hola mundo 123 // cruel")


import re

re.sub(r"[^a-zA-z_\-\s/]", "", "hola mundo 123 // cruel")


import nltk
from nltk.tokenize import word_tokenize

nltk.pos_tag(word_tokenize("hola mundo 123 // cruel"))


len(set(list("abcde")) & set(list("miou")))



